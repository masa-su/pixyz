{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder\n",
    "* Original paper: Auto-Encoding Variational Bayes (https://arxiv.org/pdf/1312.6114.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST\n",
    "root = '../data'\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambd=lambda x: x.view(-1))])\n",
    "kwargs = {'batch_size': batch_size, 'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=True, transform=transform, download=True),\n",
    "    shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=False, transform=transform),\n",
    "    shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define probability distributions\n",
    "Prior: $p(z) = \\cal N(z; \\mu=0, \\sigma^2=1)$  \n",
    "Generator: $p_{\\theta}(x|z) = \\cal B(x; \\lambda = g(z))$  \n",
    "Inference: $q_{\\phi}(z|x) = \\cal N(z; \\mu=f_\\mu(x), \\sigma^2=f_{\\sigma^2}(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal, Bernoulli\n",
    "\n",
    "x_dim = 784\n",
    "z_dim = 64\n",
    "\n",
    "\n",
    "# inference model q(z|x)\n",
    "class Inference(Normal):\n",
    "    \"\"\"\n",
    "    parameterizes q(z | x)\n",
    "    infered z follows a Gaussian distribution with mean 'loc', variance 'scale'\n",
    "    z ~ N(loc, scale)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"x\"], var=[\"z\"], name=\"q\")\n",
    "\n",
    "        self.fc1 = nn.Linear(x_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc31 = nn.Linear(512, z_dim)\n",
    "        self.fc32 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        given the observation x,\n",
    "        return the mean and variance of the Gaussian distritbution\n",
    "        \"\"\"\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}\n",
    "\n",
    "    \n",
    "# generative model p(x|z)    \n",
    "class Generator(Bernoulli):\n",
    "    \"\"\"\n",
    "    parameterizes the bernoulli(for MNIST) observation likelihood p(x | z)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"], name=\"p\")\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, x_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        given the latent variable z,\n",
    "        return the probability of Bernoulli distribution\n",
    "        \"\"\"\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "    \n",
    "p = Generator().to(device)\n",
    "q = Inference().to(device)\n",
    "\n",
    "#  prior p(z)\n",
    "# z ~ N(0, 1)\n",
    "prior = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),\n",
    "               var=[\"z\"], features_shape=[z_dim], name=\"p_{prior}\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p_{prior}(z)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p_{prior}, distribution_name=Normal,\n",
      "    var=['z'], cond_var=[], input_var=[], features_shape=torch.Size([64])\n",
      "    (loc): torch.Size([1, 64])\n",
      "    (scale): torch.Size([1, 64])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p_{prior}(z)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prior)\n",
    "print_latex(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x|z)\n",
      "Network architecture:\n",
      "  Generator(\n",
      "    name=p, distribution_name=Bernoulli,\n",
      "    var=['x'], cond_var=['z'], input_var=['z'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc3): Linear(in_features=512, out_features=784, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x|z)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p)\n",
    "print_latex(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  q(z|x)\n",
      "Network architecture:\n",
      "  Inference(\n",
      "    name=q, distribution_name=Normal,\n",
      "    var=['z'], cond_var=['x'], input_var=['x'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc31): Linear(in_features=512, out_features=64, bias=True)\n",
      "    (fc32): Linear(in_features=512, out_features=64, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$q(z|x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q)\n",
    "print_latex(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss function\n",
    "Loss function:\n",
    "\n",
    "$\\frac{1}{N} \\sum_{i=1}^{N}\\left[K L\\left(q\\left(z | x^{(i)}\\right) \\| p_{prior}(z)\\right)-\\mathbb{E}_{q\\left(z | x^{(i)}\\right)}\\left[\\log p\\left(x^{(i)} | z\\right)\\right]\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb, KullbackLeibler, Expectation as E\n",
    "\n",
    "loss = (KullbackLeibler(q, prior) - E(q, LogProb(p))).mean()\n",
    "print_latex(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define VAE model using Model Class\n",
    "- https://docs.pixyz.io/en/latest/models.html#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributions (for training): \n",
      "  p(x|z), q(z|x) \n",
      "Loss function: \n",
      "  mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right) \n",
      "Optimizer: \n",
      "  Adam (\n",
      "  Parameter Group 0\n",
      "      amsgrad: False\n",
      "      betas: (0.9, 0.999)\n",
      "      eps: 1e-08\n",
      "      lr: 0.001\n",
      "      weight_decay: 0\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.models import Model\n",
    "\n",
    "model = Model(loss=loss, distributions=[p, q],\n",
    "              optimizer=optim.Adam, optimizer_params={\"lr\":1e-3})\n",
    "print(model)\n",
    "print_latex(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test loop using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    for x, _ in tqdm(train_loader):\n",
    "        x = x.to(device)\n",
    "        loss = model.train({\"x\": x})\n",
    "        train_loss += loss\n",
    " \n",
    "    train_loss = train_loss * train_loader.batch_size / len(train_loader.dataset)\n",
    "    print('Epoch: {} Train loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    test_loss = 0\n",
    "    for x, _ in test_loader:\n",
    "        x = x.to(device)\n",
    "        loss = model.test({\"x\": x})\n",
    "        test_loss += loss\n",
    "\n",
    "    test_loss = test_loss * test_loader.batch_size / len(test_loader.dataset)\n",
    "    print('Test loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct image and generate image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstrunction(x):\n",
    "    \"\"\"\n",
    "    reconstruct image given input observation x\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # infer and sampling z using inference model q `.sample()` method\n",
    "        z = q.sample({\"x\": x}, return_all=False)\n",
    "        \n",
    "        # reconstruct image from inferred latent variable z using Generator model p `.sample_mean()` method\n",
    "        recon_batch = p.sample_mean(z).view(-1, 1, 28, 28)\n",
    "        \n",
    "        # concatenate original image and reconstructed image for comparison\n",
    "        comparison = torch.cat([x.view(-1, 1, 28, 28), recon_batch]).cpu()\n",
    "        return comparison\n",
    "\n",
    "\n",
    "def plot_image_from_latent(z_sample):\n",
    "    \"\"\"\n",
    "    generate new image given latent variable z\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # generate image from latent variable z using Generator model p `.sample_mean()` method\n",
    "        sample = p.sample_mean({\"z\": z_sample}).view(-1, 1, 28, 28).cpu()\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "dt_now = datetime.datetime.now()\n",
    "exp_time = dt_now.strftime('%Y%m%d_%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:21<00:00, 21.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 179.0260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 142.2970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:21<00:00, 22.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 130.3711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 122.8682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:21<00:00, 21.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train loss: 118.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 115.1628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:21<00:00, 22.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train loss: 113.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 111.3590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:22<00:00, 20.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train loss: 110.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 109.3695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:21<00:00, 21.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train loss: 108.1386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 107.6906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:20<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train loss: 106.6020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 106.4548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:21<00:00, 21.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train loss: 105.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 105.6179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:20<00:00, 22.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train loss: 104.1733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 104.3698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:21<00:00, 21.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train loss: 103.3093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 104.0028\n"
     ]
    }
   ],
   "source": [
    "# for visualising in TensorBoard\n",
    "import pixyz\n",
    "v = pixyz.__version__\n",
    "writer = SummaryWriter(\"runs/\" + v + \".vae\" + exp_time)\n",
    "\n",
    "# fix latent variable z for watching generative model improvement \n",
    "z_sample = 0.5 * torch.randn(64, z_dim).to(device)\n",
    "\n",
    "# set-aside observation for watching generative model improvement \n",
    "_x, _ = iter(test_loader).next()\n",
    "_x = _x.to(device)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    \n",
    "    recon = plot_reconstrunction(_x[:8])\n",
    "    sample = plot_image_from_latent(z_sample)\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss.item(), epoch)\n",
    "    writer.add_scalar('test_loss', test_loss.item(), epoch)      \n",
    "    \n",
    "    writer.add_images('Image_from_latent', sample, epoch)\n",
    "    writer.add_images('Image_reconstrunction', recon, epoch)\n",
    "elapsed_time = time.time() - start\n",
    "writer.add_scalar('Exp time second', elapsed_time)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
