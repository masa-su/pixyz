{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層生成モデルではモデルの設計=目的関数の定義\n",
    "- 深層生成モデルでは，いずれのモデルも最適化するための目的関数を明示的に設定する\n",
    "    - 自己回帰モデル・フローベースモデル: Kullback-Leiblerダイバージェンス(対数尤度)\n",
    "    - VAE: 周辺対数尤度の下界\n",
    "    - GAN: Jensen-Shannonダイバージェンス(ただし目的関数自身の更新も必要(=敵対的学習))\n",
    "- 推論，確率変数の表現の正則化なども，全て目的関数として追加する\n",
    "<img src='../tutorial_figs/vae_loss.png'>\n",
    "   \n",
    "    - 深層生成モデルではモデルの設計=目的関数の定義\n",
    "    - 従来の生成モデルとは異なり，サンプリングによる推論等は行わない\n",
    "- 確率分布を受け取って目的関数を定義できる枠組みが必要\n",
    "    - LossAPI  \n",
    "<img src='../tutorial_figs/PixyzAPI.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率分布を受け取って目的関数を定義する\n",
    "- Loss API document: https://pixyz.readthedocs.io/en/latest/losses.html#\n",
    "\n",
    "ここではDistribution APIで定義した確率分布を受け取り目的関数を定義するまでの流れを確認する  \n",
    "目的関数を定義する際には以下の項目が必要となる\n",
    "1. 尤度計算をする\n",
    "1. 確率分布の距離を計算する\n",
    "1. 期待値を計算する\n",
    "1. データ分布を考慮した計算(mean, sum)  \n",
    "\n",
    "VAEのLossではそれぞれの項目は以下のように対応\n",
    "<img src='../tutorial_figs/vae_loss_API.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossの計算\n",
    "Loss API はlossの計算を行う際入力に確率変数を必要とします(`input_var`).\n",
    "確率変数が与えられて初めてLossのあたいは計算されます.  \n",
    "\n",
    "```python\n",
    "p = DistributionAPI()\n",
    "# define the objective function receiving distribution\n",
    "loss = LossAPI(p)\n",
    "# the value of loss is calculated when input_var is feeded\n",
    "loss_value = loss.eval({'input_var': input_data})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f32201abc30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixyz module\n",
    "from pixyz.distributions import Normal\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尤度計算を行う\n",
    "ある観測値$x_1$, ...., $x_N$が得られた際，xが従うと仮定した確率分布pの尤もらしさを計算します  \n",
    "ここではxは平均0, 分散1の正規分布に従うのではないかと仮定します  \n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([5])\n",
      "    (loc): torch.Size([1, 5])\n",
      "    (scale): torch.Size([1, 5])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確率分布pを定義\n",
    "x_dim = 5\n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print(p_nor_x)\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "# xを観測\n",
    "observed_x_num = 100\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "print(observed_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "対数尤度は以下のように計算されます  \n",
    "$L=\\sum_{i=1}^{100} \\log p\\left(x_{i}\\right)$  \n",
    "PixyzではLogProbを使用することで簡単に計算できます  \n",
    "LogProbの引数にPixyz Distributionで定義した確率分布を格納し  \n",
    "観測値をLogProb.eval()で渡すことで計算が行われます  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#probability-density-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\log p(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb\n",
    "# LogProbの引数にPixyz Distributionで定義した確率分布を格納\n",
    "log_likelihood_x = LogProb(p_nor_x)\n",
    "print_latex(log_likelihood_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -7.5539,  -6.8545,  -6.4024,  -5.8851,  -6.1517,  -8.3702,  -6.7028,\n",
      "         -5.0395,  -7.4346,  -7.1497,  -5.7594,  -7.3006, -11.9857,  -5.8238,\n",
      "         -6.7561,  -5.7640,  -6.2382,  -4.9060,  -6.1076,  -8.2535,  -7.8250,\n",
      "         -7.1956,  -7.6949,  -5.2324, -11.5860,  -8.1068,  -7.1763,  -8.3332,\n",
      "        -11.4631,  -6.6297,  -6.1200, -12.2358,  -5.3402,  -7.1465,  -7.5106,\n",
      "         -7.0829,  -6.6300,  -6.1832,  -7.2049, -10.8676,  -6.8674,  -5.8339,\n",
      "         -9.1939,  -7.5965,  -8.7743,  -7.3492,  -5.2578, -10.3097,  -6.5646,\n",
      "         -4.8807,  -5.9738,  -6.2394, -10.3945,  -9.1760,  -9.2957,  -5.5627,\n",
      "         -7.1047,  -6.4066,  -6.8100,  -6.0878,  -6.8835,  -7.9132,  -5.0738,\n",
      "         -8.8378,  -6.2286,  -5.8401,  -5.9691,  -5.6857,  -7.6903,  -6.4982,\n",
      "         -7.1259,  -8.7953, -10.5572,  -5.9161,  -7.0649,  -6.1292,  -6.0871,\n",
      "         -7.2513,  -7.2517,  -7.1378,  -6.4228,  -5.5728,  -5.6155,  -5.1962,\n",
      "         -8.3940,  -7.8178,  -9.8129,  -6.1119,  -5.0492,  -8.9898,  -6.9675,\n",
      "         -8.0218, -13.9816,  -6.8575,  -5.1304,  -5.5069,  -5.0561,  -5.1264,\n",
      "         -4.8489,  -5.4876])\n",
      "observed_x_num:  100\n"
     ]
    }
   ],
   "source": [
    "# 観測値それぞれに対しての尤度が計算される\n",
    "print(log_likelihood_x.eval({'x': observed_x}))\n",
    "# observed_x_num = 100\n",
    "print('observed_x_num: ', len(log_likelihood_x.eval({'x': observed_x})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_likelihood_x.eval({'x': observed_x})には  \n",
    "$\\log p(x_{1})$, $\\log p(x_{2})$, ...., $\\log p(x_{100})$  \n",
    "の計算結果が格納されている  \n",
    "log_likelihood_x.eval({'x': observed_x})[i] = $\\log p(x_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に各要素を合計し\n",
    "$L=\\sum_{i=1}^{100} \\log p\\left(x_{i}\\right)$を計算する  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数尤度の計算結果: tensor(-715.5875)\n"
     ]
    }
   ],
   "source": [
    "# 値を合計し対数尤度を計算する\n",
    "print('対数尤度の計算結果:', log_likelihood_x.eval({'x': observed_x}).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上のようにpixyz.lossesのLogProbを用いることで対数尤度が簡単に計算できることを確認しました  \n",
    "また，定義した確率分布からp.log_prob().eval()でも同様に計算が行えます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogProb()\n",
      "tensor(-715.5875)\n",
      ".log_prob()\n",
      "tensor(-715.5875)\n"
     ]
    }
   ],
   "source": [
    "print('LogProb()')\n",
    "print(LogProb(p_nor_x).eval({'x': observed_x}).sum())\n",
    "print('.log_prob()')\n",
    "print(p_nor_x.log_prob().eval({'x': observed_x}).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more Loss API related to probability density function:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#probability-density-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確率分布の距離を計算する\n",
    "生成モデルの学習では真の分布(データ分布)$p_{data}(x)$と近いモデル分布(生成モデル)$p_{\\theta}(x)$を考え，適切な$\\theta$を求めるために分布間の距離を測ることがある \n",
    "\n",
    "VAE系ではKullback-Leiblerダイバージェンス, GAN系ではJensen-Shannonダイバージェンスといったように，確率分布間の距離を計算する  \n",
    "分布間距離の計算もPixyz Loss APIを用いれば簡単に行うことができる  \n",
    "Pixyz document:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#statistical-distance  \n",
    "https://pixyz.readthedocs.io/en/latest/losses.html#adversarial-statistical-distance\n",
    "\n",
    "ここでは例として平均0, 分散1の正規分布pと平均5, 分散0.1の正規分布qとのKullback-Leiblerダイバージェンスを計算する  \n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$  \n",
    "$q(x) = \\cal N(\\mu=5, \\sigma^2=0.1)$  \n",
    "$KL(q(x) || p(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確率分布の定義\n",
    "x_dim = 10\n",
    "# p \n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle q(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q\n",
    "q_nor_x = Normal(var=['x'], loc=torch.tensor(5.), scale=torch.tensor(0.1), features_shape=[x_dim], name='q')\n",
    "print_latex(q_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback-Leiblerダイバージェンスを計算はpixyz.lossesのKullbackLeiblerを用いる  \n",
    "KullbackLeibler()の引数に距離を測りたい分布を格納し   \n",
    ".eval()で計算が行われる  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#kullbackleibler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle D_{KL} \\left[q(x)||p(x) \\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import KullbackLeibler\n",
    "\n",
    "kl_q_p = KullbackLeibler(q_nor_x, p_nor_x)\n",
    "print_latex(kl_q_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([143.0759])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .eval で計算を行う\n",
    "kl_q_p.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more Loss API related to statistical distance: \n",
    "https://docs.pixyz.io/en/latest/losses.html#statistical-distance  \n",
    "https://docs.pixyz.io/en/latest/losses.html#adversarial-statistical-distance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 期待値を計算する\n",
    "何らかの関数について確率分布で重み付けして積分を行うのが期待値計算であるが\n",
    "Pixyzでは潜在変数のように, input_varとして与えられない変数がある場合その変数が従う確率分布で潰\n",
    "期待値の計算もLoss APIを用いれば簡単に計算できる  \n",
    "Pixyz document:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは例として  \n",
    "$q(z|x) = \\cal N(\\mu=x, \\sigma^2=1)$  \n",
    "$p(x|z) = \\cal N(\\mu=z, \\sigma^2=1)$  \n",
    "といった二つの確率分布q(z|x), p(x|z)を考え  \n",
    "$\\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right]$を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確率分布の定義\n",
    "from pixyz.distributions import Normal\n",
    "\n",
    "q_nor_z__x = Normal(loc=\"x\", scale=torch.tensor(1.), var=[\"z\"], cond_var=[\"x\"],\n",
    "           features_shape=[10], name='q') # q(z|x)\n",
    "p_nor_x__z = Normal(loc=\"z\", scale=torch.tensor(1.), var=[\"x\"], cond_var=[\"z\"],\n",
    "                    features_shape=[10]) # p(x|z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\log p(x|z)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(x|z)の対数尤度をとる\n",
    "from pixyz.losses import LogProb\n",
    "\n",
    "p_log_likelihood = LogProb(p_nor_x__z)\n",
    "print_latex(p_log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "期待値の計算はpixyz.lossesのExpectationを用いる  \n",
    "Expectation()の引数にはp, fがあり  \n",
    "期待値をとる対象の関数がfで, その関数の確率変数が従う確率分布のpで重み付けを行う  \n",
    ".eval()で計算が行われる  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import Expectation as E\n",
    "\n",
    "E_q_logprob_p = E(q_nor_z__x, LogProb(p_nor_x__z))\n",
    "print_latex(E_q_logprob_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.7006, -11.9861])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x = torch.randn(2, 10)\n",
    "E_q_logprob_p.eval({'x': sample_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about Expectatoin API:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ分布を考慮した計算(mean, sum)\n",
    "本来ならxについて期待値をとる必要があるが，データ分布は実際に与えられないためbatch方向について平均や合計といった計算を行う  \n",
    "合計や平均といった計算もLoss APIでは簡単に行うことができる  \n",
    "ここではobserved_xを訓練データとして尤度計算を行いそのmeanを計算する\n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$  \n",
    "$\\frac{1}{N} \\sum_{i=1}^N\\left[\\log p\\left(x^{(i)}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "# xを観測\n",
    "observed_x_num = 100\n",
    "x_dim = 5\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "print(observed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([5])\n",
      "    (loc): torch.Size([1, 5])\n",
      "    (scale): torch.Size([1, 5])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確率分布pを定義\n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print(p_nor_x)\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合計や平均といった計算はLoss.mean()やLoss.sum()とすることで容易に行える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle mean \\left(\\log p(x) \\right)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb\n",
    "# meanを計算する\n",
    "mean_log_likelihood_x = LogProb(p_nor_x).mean() # .mean()\n",
    "print_latex(mean_log_likelihood_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.1973)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_log_likelihood_x.eval({'x': observed_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossの組み合わせ\n",
    "PixyzではLoss同士の四則演算ができる  \n",
    "例として以下のLossをLoss同士の組み合わせで表現する  \n",
    "$\\frac{1}{N} \\sum_{i=1}^{N}\\left[\\mathbb{E}_{q\\left(z | x^{(i)}\\right)}\\left[\\log p\\left(x^{(i)} | z\\right)\\right]-K L\\left(q\\left(z | x^{(i)}\\right) \\| p(z)\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確率分布の定義\n",
    "from pixyz.distributions import Normal\n",
    "\n",
    "# p(x|z)\n",
    "p_nor_x__z = Normal(loc=\"z\", scale=torch.tensor(1.), var=[\"x\"], cond_var=[\"z\"],\n",
    "                    features_shape=[10])\n",
    "\n",
    "# p(z)\n",
    "p_nor_z = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.), var=[\"z\"],\n",
    "                    features_shape=[10])\n",
    "\n",
    "# q(z|x)\n",
    "q_nor_z__x = Normal(loc=\"x\", scale=torch.tensor(1.), var=[\"z\"], cond_var=[\"x\"],\n",
    "           features_shape=[10], name='q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle mean \\left(- D_{KL} \\left[q(z|x)||p(z) \\right] + \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lossの定義\n",
    "from pixyz.losses import LogProb\n",
    "from pixyz.losses import Expectation as E\n",
    "from pixyz.losses import KullbackLeibler\n",
    "\n",
    "# 対数尤度\n",
    "logprob_p_x__z = LogProb(p_nor_x__z)# input_var: x, z\n",
    "\n",
    "# 期待値E\n",
    "E_q_z__x_logprob_p__z = E(q_nor_z__x, logprob_p_x__z)# input_car: x(z is not needed because of Expectation)\n",
    "\n",
    "# KLダイバージェンス\n",
    "KL_q_z__x_p_z = KullbackLeibler(q_nor_z__x, p_nor_z)\n",
    "\n",
    "# Lossの引き算\n",
    "total_loss = E_q_z__x_logprob_p__z - KL_q_z__x_p_z# input_var: x(E_q_z__x_logprob_p__z needs x as input_var)\n",
    "\n",
    "# Lossのmean\n",
    "total_loss = total_loss.mean()\n",
    "\n",
    "# Lossの確認\n",
    "print_latex(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-18.9965)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lossの計算\n",
    "# xを観測\n",
    "observed_x_num = 100\n",
    "x_dim = 10\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "\n",
    "# 観測したxのLossを計算\n",
    "total_loss.eval({'x': observed_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上のようにPixyz Loss API同士の四則演算で柔軟にLossが定義でき，数式から実装までが直感的に行えることが確認できた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss API(ELBO)\n",
    "Pixyz Loss APIでは以下のようなLossについても実装がある\n",
    "\n",
    "周辺尤度下界 ELBO: https://docs.pixyz.io/en/latest/losses.html#lower-bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Tutorial\n",
    "ModelAPITutorial.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
