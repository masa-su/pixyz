{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層生成モデルの学習方法は目的関数を定義して勾配降下法で学習\n",
    "- 深層生成モデルでは，既存のようなサンプリング等によっての学習は行わない\n",
    "\n",
    "\n",
    "<img src='../tutorial_figs/PixyzAPI.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的関数と最適化アルゴリズムが独立に設定できる枠組み(Model API)\n",
    "- Model API document: https://docs.pixyz.io/en/v0.0.4/models.html  \n",
    "\n",
    "ここでは定義した確率分布と目的関数を受け取り，モデルの学習を行う流れを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x118f9b690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "batch_size = 256\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST datasetの準備\n",
    "root = '../data'\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambd=lambda x: x.view(-1))])\n",
    "kwargs = {'batch_size': batch_size, 'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=True, transform=transform, download=True),\n",
    "    shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=False, transform=transform),\n",
    "    shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確率分布の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal, Bernoulli\n",
    "\n",
    "x_dim = 784\n",
    "z_dim = 64\n",
    "\n",
    "# inference model q(z|x)\n",
    "class Inference(Normal):\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"x\"], var=[\"z\"], name=\"q\")\n",
    "\n",
    "        self.fc1 = nn.Linear(x_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc31 = nn.Linear(512, z_dim)\n",
    "        self.fc32 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}\n",
    "\n",
    "    \n",
    "# generative model p(x|z)    \n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"], name=\"p\")\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, x_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "    \n",
    "gen_ber_x__z = Generator().to(device)\n",
    "infer_nor_z__x = Inference().to(device)\n",
    "\n",
    "prior_nor_z = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),\n",
    "               var=[\"z\"], features_shape=[z_dim], name=\"p_{prior}\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lossの定義\n",
    "from pixyz.losses import LogProb\n",
    "from pixyz.losses import StochasticReconstructionLoss\n",
    "from pixyz.losses import Expectation as E\n",
    "from pixyz.losses import KullbackLeibler\n",
    "from pixyz.utils import print_latex\n",
    "\n",
    "# 対数尤度\n",
    "logprob_gen_x__z = LogProb(gen_ber_x__z)\n",
    "\n",
    "# 期待値E\n",
    "E_infer_z__x_logprob_gen_x__z = E(infer_nor_z__x, logprob_gen_x__z)\n",
    "\n",
    "# KLダイバージェンス\n",
    "KL_infer_nor_z__x_prior_nor_z = KullbackLeibler(infer_nor_z__x, prior_nor_z)\n",
    "\n",
    "# Lossの引き算\n",
    "total_loss = KL_infer_nor_z__x_prior_nor_z - E_infer_z__x_logprob_gen_x__z\n",
    "\n",
    "# Lossのmean\n",
    "total_loss = total_loss.mean()\n",
    "\n",
    "\n",
    "# Lossの確認\n",
    "print_latex(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelAPIに確率分布とLossを渡し，最適化アルゴリズムを設定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pixyz.modelsのModelを呼び出して使用\n",
    "主な引数はloss, distributions, optimizer, optimzer_paramsで，それぞれには以下のように格納します\n",
    "- loss: pixyz.lossesを使用して定義した目的関数のLossを格納\n",
    "- distributions: pixyz.distributionを使用して定義した，学習を行う確率分布を格納\n",
    "- optimizer, optimizer_params: 最適化アルゴリズム，そのパラメータを格納  \n",
    "\n",
    "For more details about Model: https://docs.pixyz.io/en/v0.0.4/_modules/pixyz/models/model.html#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pixyz.models import Model\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam\n",
    "optimizer_params = {'lr': 1e-3}\n",
    "\n",
    "vae_model = Model(loss=total_loss, \n",
    "                     distributions=[gen_ber_x__z, infer_nor_z__x],\n",
    "                     optimizer=optimizer,\n",
    "                     optimizer_params=optimizer_params\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でModelの定義が完了した  \n",
    "目的関数の設定と，最適化アルゴリズムの設定が独立に行えたことを確認できた  \n",
    "次に実際にtrainメソッドについて確認し実際に学習を行う  \n",
    "Model Classのtrainメソッドでは以下の処理を行なっている  \n",
    "source code: https://docs.pixyz.io/en/v0.0.4/_modules/pixyz/models/model.html#Model.train\n",
    "1. 観測データであるxを受け取り(.train({\"x\": x}))\n",
    "2. Lossを計算し\n",
    "3. 1stepパラメーターの更新を行い\n",
    "4. Lossを出力  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(self, train_x={}, **kwargs):\n",
    "        self.distributions.train()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_cls.estimate(train_x, **kwargs)\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # update params\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 199.60440063476562 \n",
      "Epoch 1, Loss 147.97647094726562 \n",
      "Epoch 2, Loss 128.66696166992188 \n"
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "for epoch in range(3):\n",
    "    train_loss = 0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        loss = vae_model.train({\"x\": x})\n",
    "        train_loss += loss\n",
    "    train_loss = train_loss * train_loader.batch_size / len(train_loader.dataset)\n",
    "    print('Epoch {}, Loss {} '.format(epoch, train_loss))\n",
    "    epoch_loss.append(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で学習を行えることを確認した  \n",
    "Pixyzでは高度なModelAPIとしてVAE, GAN Modelを用意しており，ただ入力データの変更やDNNのネットワークアーキテクチャーを変更したいだけの場合は高度なModel APIを使用することで簡単に実装することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高度なModel APIの使用\n",
    "高度なModel APIを使用すると，簡単にモデルを定義することができる\n",
    "必要となる実装は  \n",
    "- 確率分布の定義\n",
    "- (追加的な目的関数の設定)\n",
    "- 最適化アルゴリズムの選択  \n",
    "\n",
    "である, ここではVAEモデルを例に高度なModel APIを使用して実装する流れを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal, Bernoulli\n",
    "from pixyz.losses import KullbackLeibler\n",
    "# 高度なModel API VAE\n",
    "from pixyz.models import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確率分布の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dim = 784\n",
    "z_dim = 64\n",
    "\n",
    "\n",
    "# inference model q(z|x)\n",
    "class Inference(Normal):\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"x\"], var=[\"z\"], name=\"q\")\n",
    "\n",
    "        self.fc1 = nn.Linear(x_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc31 = nn.Linear(512, z_dim)\n",
    "        self.fc32 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}\n",
    "\n",
    "    \n",
    "# generative model p(x|z)    \n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"], name=\"p\")\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, x_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "    \n",
    "p = Generator().to(device)\n",
    "q = Inference().to(device)\n",
    "\n",
    "prior = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),\n",
    "               var=[\"z\"], features_shape=[z_dim], name=\"p_{prior}\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目的関数の正則化項の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$D_{KL} \\left[q(z|x)||p_{prior}(z) \\right]$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl = KullbackLeibler(q, prior)\n",
    "print_latex(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE modelの使用・最適化アルゴリズムの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(encoder=q, decoder=p, regularizer=kl, \n",
    "            optimizer=optim.Adam, optimizer_params={\"lr\":1e-3})\n",
    "print_latex(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        loss = model.train({\"x\": x})\n",
    "        train_loss += loss\n",
    " \n",
    "    train_loss = train_loss * train_loader.batch_size / len(train_loader.dataset)\n",
    "    print('Epoch: {} Train loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 201.2876\n",
      "Epoch: 2 Train loss: 147.1453\n",
      "Epoch: 3 Train loss: 128.1311\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "train_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    train_losses.append(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more higher model API\n",
    "- Pre-implementation models: https://docs.pixyz.io/en/v0.0.4/models.html#pre-implementation-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixyzの実用例\n",
    "さらに複雑なモデルの実装例は以下のリンクにある\n",
    "- Pixyz examples: https://github.com/masa-su/pixyz/tree/master/examples\n",
    "- Pixyzoo: https://github.com/masa-su/pixyzoo\n",
    "\n",
    "\n",
    "1. Distribution APIで柔軟にニューラルネットワークを用いた確率分布を定義\n",
    "1. Loss APIではDistribution APIで定義した確率分布をもとに, Lossの設計を行う\n",
    "1. Model APIではLoss APIで定義した目的関数と, 学習する確率分布を受け取り，最適化アルゴリズムを設定\n",
    "1. Model APIで定義したモデルで学習を行う\n",
    "\n",
    "という基本的な実装の流れはどのモデルでも変わらない"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
